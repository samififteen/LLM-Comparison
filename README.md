# ğŸ¤– LLM Comparison

LLM Comparison is a project focused on analyzing and comparing different **Large Language Models (LLMs)** based on their performance, response quality, and behavior across a variety of tasks.  
The goal is to provide clear insights into the strengths and limitations of each model through practical evaluation.

---

## ğŸ“Œ Project Overview

Large Language Models have become central to modern AI applications. This project aims to:
- Compare multiple LLMs using standardized prompts
- Analyze response quality, coherence, and relevance
- Evaluate real-world usability and performance
- Provide an easy-to-understand comparison framework

---

## ğŸ›  Tech Stack

- **Python** â€” Core implementation
- **LLM APIs / Models** 
- **Streamlit**

---

## ğŸ“ Project Structure

```
/LLM-Comparision
|-- /models # LLM integration logic
|-- /utils # Helper functions
|-- app.py # Entry point
|-- requirements.txt # Dependencies
|-- README.md # Project documentation
```

## ğŸš€ Getting Started

### 1. Clone the repository
```
git clone https://github.com/samififteen/LLM-Comparison.git
```

### 2. Navigate to the project directory
```
cd LLM-Comparison
```

### 3. Create a virtual environment (recommended)
```
python -m venv env

```

### 4. Install dependencies
```
pip install -r requirements.txt
```

### 5. Run the project
```
python app.py
```
